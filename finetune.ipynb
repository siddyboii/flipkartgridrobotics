{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5863298,"sourceType":"datasetVersion","datasetId":3371317},{"sourceId":9553973,"sourceType":"datasetVersion","datasetId":5821463},{"sourceId":9595820,"sourceType":"datasetVersion","datasetId":5853408},{"sourceId":9610982,"sourceType":"datasetVersion","datasetId":5864487},{"sourceId":9613412,"sourceType":"datasetVersion","datasetId":5866294},{"sourceId":9614097,"sourceType":"datasetVersion","datasetId":5866820},{"sourceId":9643815,"sourceType":"datasetVersion","datasetId":5889348},{"sourceId":9648352,"sourceType":"datasetVersion","datasetId":5892756}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Pytorch & other libraries\n%pip install \"torch==2.4.0\" tensorboard pillow\n\n \n# Install Hugging Face libraries\n%pip install  --upgrade \\\n  \"transformers==4.45.1\" \\\n  \"datasets==3.0.1\" \\\n  \"accelerate==0.34.2\" \\\n  \"evaluate==0.4.3\" \\\n  \"bitsandbytes==0.44.0\" \\\n  \"trl==0.11.1\" \\\n  \"peft==0.13.0\" \\\n  \"qwen-vl-utils\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nwith open('/kaggle/input/new-data-set/modified_file.json', 'r') as file:\n    dataset = json.load(file)\n    \n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\n \nlogin(\n  token=\"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\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForVision2Seq, AutoProcessor, BitsAndBytesConfig\n \n# Hugging Face model id\nmodel_id = \"Qwen/Qwen2-VL-7B-Instruct\" \n \n# BitsAndBytesConfig int-4 config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, bnb_4bit_use_double_quant=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16\n)\n \n# Load model and tokenizer\nmodel = AutoModelForVision2Seq.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    # attn_implementation=\"flash_attention_2\", # not supported for training\n    torch_dtype=torch.bfloat16,\n    quantization_config=bnb_config\n)\n# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n# \n# model = torch.nn.DataParallel(model, device_ids = [0,1]).to(device)\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n# model.load_adapter(\"/kaggle/working/qwen2-7b-instruct-product_descripter_v1\") # load the adapter and activate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Get the messages from your dataset\nmessages = dataset[2][\"messages\"]\n\n# Create your custom system message\n\n\n# Insert the system message at the beginning of the messages list\nmessages_with_system = messages\n\n# Apply the chat template using the modified messages list\ntext = processor.apply_chat_template(\n    messages_with_system, tokenize=False, add_generation_prompt=False\n)\nprint(text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from peft import LoraConfig\n \n# LoRA config based on QLoRA paper & Sebastian Raschka experiment\npeft_config = LoraConfig(\n        lora_alpha=16,\n        lora_dropout=0.05,\n        r=8,\n        bias=\"none\",\n        target_modules=[\"q_proj\", \"v_proj\"],\n        task_type=\"CAUSAL_LM\", \n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTConfig\nfrom transformers import Qwen2VLProcessor\nfrom qwen_vl_utils import process_vision_info\n \nargs = SFTConfig(\n    output_dir=\"llama3.1_v1\", # directory to save and repository id\n    num_train_epochs=10,                     # number of training epochs\n    per_device_train_batch_size=2,          # batch size per device during training\n    gradient_accumulation_steps=8,          # number of steps before performing a backward/update pass\n    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n    logging_steps=4,                       # log every 10 steps\n    save_strategy=\"epoch\",                  # save checkpoint every epoch\n    learning_rate=4e-4,                     # learning rate, based on QLoRA paper\n    bf16=True,                              # use bfloat16 precision\n    tf32=False,                              # use tf32 precision\n    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n    push_to_hub=False,                       # push model to hub\n    report_to=\"tensorboard\",                # report metrics to tensorboard\n    gradient_checkpointing_kwargs = {\"use_reentrant\": False}, # use reentrant checkpointing\n    dataset_text_field=\"\", # need a dummy field for collator\n    dataset_kwargs = {\"skip_prepare_dataset\": True} # important for collator\n)\nargs.remove_unused_columns=False\n \n# Create a data collator to encode text and image pairs\nfrom PIL import Image\nimport torchvision.transforms as transforms\n\n# Define your image transformation (adjust as necessary for your use case)\nimage_transform = transforms.Compose([\n    transforms.Resize((800, 800)),  # Resize to the desired size\n    transforms.ToTensor(),           # Convert to tensor\n    # Add any additional transformations if necessary\n])\n\ndef collate_fn(examples):\n    # Get the texts from user and assistant roles\n    texts = []\n    for example in examples:\n        system_message = next((msg[\"content\"] for msg in example[\"messages\"] if msg[\"role\"] == \"system\"), \"\")\n        user_message = next((msg[\"content\"] for msg in example[\"messages\"] if msg[\"role\"] == \"user\"), \"\")\n        assistant_message = next((msg[\"content\"] for msg in example[\"messages\"] if msg[\"role\"] == \"assistant\"), \"\")\n        combined_text = f\"{system_message}{user_message} {assistant_message}\"\n        texts.append(combined_text)\n\n    # Load and transform the images\n    image_inputs = []\n    for example in examples:\n        for image_path in example[\"images\"]:\n            try:\n                image = Image.open(image_path).convert(\"RGB\")  # Open the image and convert to RGB\n                image = image_transform(image)  # Apply transformations\n                image_inputs.append(image)  # Add the transformed image to the list\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {e}\")\n                image_inputs.append(None)  # Append None or handle as needed\n\n    # Stack images into a tensor if applicable\n    image_inputs = torch.stack([img for img in image_inputs if img is not None])  # Remove any None values\n\n    # Tokenize the texts and process the images\n    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n\n    # The labels are the input_ids, and we mask the padding tokens in the loss computation\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens\n\n    # Ignore the image token index in the loss computation (model specific)\n    if isinstance(processor, Qwen2VLProcessor):\n        image_tokens = [151652, 151653, 151655]\n    else:\n        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n\n    for image_token_id in image_tokens:\n        labels[labels == image_token_id] = -100\n    batch[\"labels\"] = labels\n\n    return batch\n\n\n# def collate_fn(examples):\n#     # Get the texts and images, and apply the chat template\n#     texts = [processor.apply_chat_template(example[\"messages\"], tokenize=False) for example in examples]\n#     image_inputs = [process_vision_info(example[\"messages\"])[0] for example in examples]\n \n#     # Tokenize the texts and process the images\n#     batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n \n#     # The labels are the input_ids, and we mask the padding tokens in the loss computation\n#     labels = batch[\"input_ids\"].clone()\n#     labels[labels == processor.tokenizer.pad_token_id] = -100  #\n#     # Ignore the image token index in the loss computation (model specific)\n#     if isinstance(processor, Qwen2VLProcessor):\n#         image_tokens = [151652,151653,151655]\n#     else: \n#         image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n#     for image_token_id in image_tokens:\n#         labels[labels == image_token_id] = -100\n#     batch[\"labels\"] = labels\n \n#     return batch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom PIL import Image\n\ndef collate_fn(examples):\n    texts = []\n    image_inputs = []\n\n    for example in examples:\n        # Get and combine text from 'user' and 'assistant'\n        user_message = next((msg[\"content\"] for msg in example[\"messages\"] if msg[\"role\"] == \"user\"), \"\")\n        assistant_message = next((msg[\"content\"] for msg in example[\"messages\"] if msg[\"role\"] == \"assistant\"), \"\")\n        combined_text = f\"{user_message} {assistant_message}\"\n        texts.append(combined_text)\n\n        # Process images in each example\n        example_images = []\n        for image_path in example[\"images\"]:\n            try:\n                image = Image.open(image_path).convert(\"RGB\")\n                image = image_transform(image)  # Assuming this is defined elsewhere\n                example_images.append(image)\n            except Exception as e:\n                print(f\"Error loading image {image_path}: {e}\")\n                example_images.append(None)  # Handle missing images\n\n        # Remove None entries and validate alignment\n        valid_images = [img for img in example_images if img is not None]\n        if valid_images:\n            image_inputs.extend(valid_images)\n\n    # Stack images, ensure shape consistency\n    if image_inputs:\n        image_inputs = torch.stack(image_inputs)\n    else:\n        image_inputs = None\n\n    # Check if text and image counts match\n    if image_inputs is not None and len(texts) != len(image_inputs):\n        raise ValueError(f\"Mismatch in texts ({len(texts)}) and images ({len(image_inputs)})\")\n\n    # Process text and images using the processor\n    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n\n    # Prepare labels with padding tokens ignored\n    labels = batch[\"input_ids\"].clone()\n    labels[labels == processor.tokenizer.pad_token_id] = -100\n\n    # Ignore image token IDs in loss computation (adjust based on processor type)\n    if isinstance(processor, Qwen2VLProcessor):\n        image_tokens = [151652, 151653, 151655]\n    else:\n        image_tokens = [processor.tokenizer.convert_tokens_to_ids(processor.image_token)]\n\n    for image_token_id in image_tokens:\n        labels[labels == image_token_id] = -100\n    batch[\"labels\"] = labels\n\n    return batch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\n \ntrainer = SFTTrainer(\n    model=model,\n    args=args,\n    train_dataset=dataset,\n    data_collator=collate_fn,\n    dataset_text_field=\"\", # needs dummy value\n    peft_config=peft_config,\n    tokenizer=processor.tokenizer,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configure FSDP auto-wrap policy\n# fsdp_plugin = trainer.accelerator.state.fsdp_plugin\n# fsdp_plugin.auto_wrap_policy = fsdp_auto_wrap_policy(trainer.model)\n\n# Start training\ntrainer.train()\n\n# Save the model\ntrainer.save_model(args.output_dir)\n\n# # start training, the model will be automatically saved to the hub and the output directory\n# trainer.train()\n \n# # save model \n# trainer.save_model(args.output_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}