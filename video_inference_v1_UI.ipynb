{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee96bb841d646ccb3cb95fff0d82f85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import gradio as gr\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "\n",
    "# Model and directories\n",
    "output_cropped_dir = 'saved_frames'  # Output directory for cropped frames\n",
    "adapter_path = \"/teamspace/studios/this_studio/newdescripterckp/checkpoint-241\"\n",
    "model_path = \"/teamspace/studios/this_studio/video_v2.pt\"\n",
    "threshold = 0.4\n",
    "\n",
    "# Set up YOLO model\n",
    "yolo_model = YOLO(model_path)\n",
    "\n",
    "# Set up Qwen model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/teamspace/studios/this_studio/newdescripterckp\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", cache_dir=\"/teamspace/studios/this_studio/newdescripterckp\", max_pixels=1080*28*28)\n",
    "model.load_adapter(adapter_path)  # Load adapter and activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://24ac92ace7650ee2ff.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://24ac92ace7650ee2ff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 product, 75.3ms\n",
      "Speed: 5.8ms preprocess, 75.3ms inference, 325.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 0 has 1 objects.\n",
      "\n",
      "0: 384x640 1 product, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 30 has 1 objects.\n",
      "\n",
      "0: 384x640 1 product, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 60 has 1 objects.\n",
      "\n",
      "0: 384x640 2 products, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 90 has 2 objects.\n",
      "\n",
      "0: 384x640 2 products, 6.7ms\n",
      "Speed: 1.4ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 120 has 2 objects.\n",
      "\n",
      "0: 384x640 3 products, 6.6ms\n",
      "Speed: 1.4ms preprocess, 6.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 150 has 3 objects.\n",
      "\n",
      "0: 384x640 4 products, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 180 has 4 objects.\n",
      "\n",
      "0: 384x640 4 products, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 210 has 4 objects.\n",
      "\n",
      "0: 384x640 4 products, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 240 has 4 objects.\n",
      "\n",
      "0: 384x640 5 products, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 270 has 5 objects.\n",
      "\n",
      "0: 384x640 5 products, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 300 has 5 objects.\n",
      "\n",
      "0: 384x640 5 products, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 330 has 5 objects.\n",
      "\n",
      "0: 384x640 6 products, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 360 has 6 objects.\n",
      "\n",
      "0: 384x640 6 products, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 390 has 6 objects.\n",
      "\n",
      "0: 384x640 6 products, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 420 has 6 objects.\n",
      "\n",
      "0: 384x640 7 products, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Frame 450 has 7 objects.\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directories exist\n",
    "if not os.path.exists(output_cropped_dir):\n",
    "    os.makedirs(output_cropped_dir)\n",
    "\n",
    "# Function to reduce bounding box and mask\n",
    "def reduce_bounding_box(x1, y1, x2, y2, reduction_factor=0.05):\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    x1_new = x1 + int(reduction_factor * width)\n",
    "    y1_new = y1 + int(reduction_factor * height)\n",
    "    x2_new = x2 - int(reduction_factor * width)\n",
    "    y2_new = y2 - int(reduction_factor * height)\n",
    "    return x1_new, y1_new, x2_new, y2_new\n",
    "\n",
    "def mask_reduced_bounding_box(frame, x1, y1, x2, y2):\n",
    "    frame[int(y1):int(y2), int(x1):int(x2)] = 0  # Set the reduced box area to zero (black)\n",
    "    return frame\n",
    "\n",
    "# Function to process each object and run Qwen inference\n",
    "def process_cropped_object(cropped_pil_image, frame_count, object_count):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": cropped_pil_image},\n",
    "                {\"type\": \"text\", \"text\": \"Identify the brand name, product type, expiry date, manufacturing date, quantity only.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Prepare input for inference\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output from the Qwen model\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "\n",
    "     # Format the output nicely for each object\n",
    "    formatted_output = (\n",
    "        f\"Inference Output for Frame {frame_count}, Object {object_count}:\\n\"\n",
    "        f\"----------------------------------------------------\\n\"\n",
    "        f\"{output_text[0]}\\n\"\n",
    "        f\"----------------------------------------------------\\n\"\n",
    "    )\n",
    "\n",
    "    # Print the output after every inference\n",
    "    # print(formatted_output)\n",
    "\n",
    "    return formatted_output  # Returning formatted output for further use\n",
    "\n",
    "    # Return the output\n",
    "    # return f\"Inference Output for Frame {frame_count}, Object {object_count}:\\n{output_text[0]}\"\n",
    "\n",
    "# Function to run YOLO on a frame and count the number of detected objects\n",
    "def count_objects_in_frame(frame):\n",
    "    results = yolo_model(frame)[0]\n",
    "    object_count = len(results.boxes.data.tolist())\n",
    "    return object_count, results\n",
    "\n",
    "# Function to process video and filter frames with the most detected objects\n",
    "def process_video_and_filter_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        return \"Error: Could not open video.\"\n",
    "    \n",
    "    video_fps = cap.get(cv2.CAP_PROP_FPS)  # Original video FPS\n",
    "    frame_interval = int(video_fps)\n",
    "\n",
    "    frame_count = 0\n",
    "    max_objects = 0\n",
    "    selected_frame = None\n",
    "    selected_results = None\n",
    "    output_text = \"\"\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit the loop if there are no more frames\n",
    "\n",
    "        if frame_count % frame_interval == 0:\n",
    "            object_count, yolo_results = count_objects_in_frame(frame)\n",
    "            print(f\"Frame {frame_count} has {object_count} objects.\")\n",
    "\n",
    "            if object_count > max_objects:\n",
    "                max_objects = object_count\n",
    "                selected_frame = frame.copy()\n",
    "                selected_results = yolo_results\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    if selected_frame is not None and selected_results is not None:\n",
    "        output_text = process_frame_with_max_objects(selected_frame, selected_results)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Function to process the frame with the most detected objects, crop them, and run inference one by one\n",
    "def process_frame_with_max_objects(frame, results):\n",
    "    object_count = 0\n",
    "    output_text = \"\"\n",
    "\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "        if score > threshold:\n",
    "            object_count += 1\n",
    "\n",
    "            # Reduce the bounding box before cropping\n",
    "            x1_new, y1_new, x2_new, y2_new = reduce_bounding_box(x1, y1, x2, y2)\n",
    "\n",
    "            cropped_image = frame[int(y1_new):int(y2_new), int(x1_new):int(x2_new)]\n",
    "            cropped_pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Run Qwen inference on the cropped image, one by one\n",
    "            output_text += process_cropped_object(cropped_pil_image, \"max_frame\", object_count)\n",
    "\n",
    "            # Mask the reduced bounding box area in the frame\n",
    "            frame = mask_reduced_bounding_box(frame, x1_new, y1_new, x2_new, y2_new)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Gradio function to process video and display results\n",
    "def run_inference_on_video(video_file):\n",
    "    output_text = process_video_and_filter_frames(video_file)\n",
    "    return output_text\n",
    "\n",
    "# Gradio UI\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Video Inference with YOLO and Qwen\")\n",
    "    \n",
    "    video_input = gr.Video(label=\"Upload Video\")\n",
    "    output_textbox = gr.Textbox(label=\"Inference Output\")\n",
    "    \n",
    "    submit_button = gr.Button(\"Run Inference\")\n",
    "    submit_button.click(run_inference_on_video, inputs=video_input, outputs=output_textbox)\n",
    "\n",
    "# Launch Gradio UI\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
