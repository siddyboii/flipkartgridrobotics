{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    " \n",
    "login(\n",
    "  \n",
    "  \n",
    "  token=\"\", # ADD YOUR TOKEN HERE\n",
    "  add_to_git_credential=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5801411bc70e4415b269ce7bee2dfb69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import cv2\n",
    "import csv\n",
    "import time\n",
    "from PIL import Image\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from flash_attn import flash_attn_qkvpacked_func, flash_attn_func\n",
    "\n",
    "# Model and directories\n",
    "image_dir = '/teamspace/studios/this_studio/Imagesflipkart'\n",
    "output_cropped_dir = '/teamspace/studios/this_studio/cropped_images'\n",
    "output_csv_path = 'detections_output.csv'\n",
    "adapter_path = \"/teamspace/studios/this_studio/newdescripterckp/checkpoint-241\"\n",
    "model_path = \"/teamspace/studios/this_studio/best.pt\"\n",
    "threshold = 0.4\n",
    "\n",
    "# Set up YOLO model\n",
    "yolo_model = YOLO(model_path)\n",
    "\n",
    "# Set up Qwen model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/teamspace/studios/this_studio/newdescripterckp\"\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", cache_dir=\"/teamspace/studios/this_studio/newdescripterckp\", max_pixels=720*28*28)\n",
    "model.load_adapter(adapter_path)  # Load adapter and activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://1f21cd1f6edd07b0d9.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://1f21cd1f6edd07b0d9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 product, 73.0ms\n",
      "Speed: 2.6ms preprocess, 73.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 product, 71.6ms\n",
      "Speed: 2.4ms preprocess, 71.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_image(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    output_text = \"\"\n",
    "    \n",
    "    # Start YOLO inference\n",
    "    start_yolo_time = time.time()\n",
    "    results = yolo_model(image)[0]\n",
    "    end_yolo_time = time.time()\n",
    "    yolo_inference_time = end_yolo_time - start_yolo_time\n",
    "    \n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "        if score > threshold:\n",
    "            # Crop detected object\n",
    "            cropped_image = image[int(y1):int(y2), int(x1):int(x2)]\n",
    "            cropped_pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Start Qwen inference\n",
    "            start_qwen_time = time.time()\n",
    "            qwen_output = process_cropped_image_with_qwen(cropped_pil_image)\n",
    "            end_qwen_time = time.time()\n",
    "            qwen_inference_time = end_qwen_time - start_qwen_time\n",
    "\n",
    "            # Append the output\n",
    "            output_text += f\"**Qwen Output:** {qwen_output}\\n\"\n",
    "            # output_text += f\"**YOLO Inference Time:** {yolo_inference_time:.2f} seconds\\n\"\n",
    "            # output_text += f\"**Qwen Inference Time:** {qwen_inference_time:.2f} seconds\\n\\n\"\n",
    "\n",
    "    return output_text\n",
    "\n",
    "def process_cropped_image_with_qwen(cropped_pil_image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": cropped_pil_image},\n",
    "                {\"type\": \"text\", \"text\": \"Identify the brand name, product type, expiry date, manufacturing date, quantity only.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output from the Qwen model\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=70)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return output_text[0]\n",
    "\n",
    "# Gradio UI\n",
    "def run_gradio_interface(image):\n",
    "    output = process_image(image)\n",
    "    return output\n",
    "\n",
    "# Define Gradio interface with image input and text output\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"## Product Inference with YOLO and Qwen\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Upload an image of a product**, and this tool will run object detection with the YOLO model \n",
    "        to identify the region of interest. Then, it will use the Qwen model to describe key product details, \n",
    "        including the brand name, product type, and quantity. The YOLO and Qwen inference times will also be displayed.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define the layout for the interface\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")\n",
    "            submit_button = gr.Button(\"Run Inference\")\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_box = gr.Textbox(label=\"Processed Output\", lines=10, max_lines=20)\n",
    "\n",
    "    # Add interaction to trigger inference\n",
    "    submit_button.click(fn=run_gradio_interface, inputs=image_input, outputs=output_box)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://d8c2ab7f9944665c1d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://d8c2ab7f9944665c1d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x384 1 product, 73.8ms\n",
      "Speed: 8.3ms preprocess, 73.8ms inference, 464.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x480 1 product, 73.8ms\n",
      "Speed: 4.4ms preprocess, 73.8ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x384 1 product, 70.6ms\n",
      "Speed: 1.6ms preprocess, 70.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n"
     ]
    }
   ],
   "source": [
    "# Function to draw bounding boxes on the image\n",
    "def draw_bounding_boxes(image, results):\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "        if score > threshold:\n",
    "            # Draw rectangle for the bounding box\n",
    "            cv2.rectangle(image, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 4)\n",
    "            # Add label\n",
    "            cv2.putText(image, f\"Object {int(class_id)}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    return image\n",
    "\n",
    "# Function to process the image with YOLO and Qwen\n",
    "def process_image(image):\n",
    "    image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    # image_bgr = cv2.resize(image_bgr,(1080,1080))\n",
    "    output_text = \"\"\n",
    "    \n",
    "    # Start YOLO inference\n",
    "    start_yolo_time = time.time()\n",
    "    results = yolo_model(image_bgr)[0]\n",
    "    end_yolo_time = time.time()\n",
    "    yolo_inference_time = end_yolo_time - start_yolo_time\n",
    "    \n",
    "    # Draw bounding boxes on the image\n",
    "    image_with_boxes = draw_bounding_boxes(image_bgr.copy(), results)\n",
    "    image_with_boxes_rgb = cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2RGB)  # Convert back to RGB for display\n",
    "\n",
    "    for result in results.boxes.data.tolist():\n",
    "        x1, y1, x2, y2, score, class_id = result\n",
    "\n",
    "        if score > threshold:\n",
    "            # Crop detected object\n",
    "            cropped_image = image_bgr[int(y1):int(y2), int(x1):int(x2)]\n",
    "            cropped_pil_image = Image.fromarray(cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "            # Start Qwen inference\n",
    "            start_qwen_time = time.time()\n",
    "            qwen_output = process_cropped_image_with_qwen(cropped_pil_image)\n",
    "            end_qwen_time = time.time()\n",
    "            qwen_inference_time = end_qwen_time - start_qwen_time\n",
    "\n",
    "            # Append the output\n",
    "            output_text += f\"**Qwen Output:** {qwen_output}\\n\"\n",
    "            # output_text += f\"**YOLO Inference Time:** {yolo_inference_time:.2f} seconds\\n\"\n",
    "            # output_text += f\"**Qwen Inference Time:** {qwen_inference_time:.2f} seconds\\n\\n\"\n",
    "\n",
    "    return image_with_boxes_rgb, output_text\n",
    "\n",
    "def process_cropped_image_with_qwen(cropped_pil_image):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\", \"image\": cropped_pil_image},\n",
    "                {\"type\": \"text\", \"text\": \"Identify the brand name, product type, expiry date, manufacturing date, quantity only.\"}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output from the Qwen model\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=70)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return output_text[0]\n",
    "\n",
    "# Gradio UI\n",
    "def run_gradio_interface(image):\n",
    "    image_with_boxes, output_text = process_image(image)\n",
    "    return image_with_boxes, output_text\n",
    "\n",
    "# Define Gradio interface with image input, image output, and text output\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"## Product Inference with YOLO and Qwen\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Upload an image of a product**, and this tool will run object detection with the YOLO model \n",
    "        to identify the region of interest. It will display the bounding boxes on the image. \n",
    "        Then, it will use the Qwen model to describe key product details, including the brand name, \n",
    "        product type, and quantity. The YOLO and Qwen inference times will also be displayed.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define the layout for the interface\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")  # Image upload box\n",
    "            submit_button = gr.Button(\"Run Inference\")  # Submit button\n",
    "        \n",
    "        with gr.Column():\n",
    "            image_output = gr.Image(label=\"Image with Bounding Boxes\")  # Image output with bounding boxes\n",
    "            output_box = gr.Textbox(label=\"Processed Output\", lines=10, max_lines=20)  # Text output\n",
    "\n",
    "    # Add interaction to trigger inference\n",
    "    submit_button.click(fn=run_gradio_interface, inputs=image_input, outputs=[image_output, output_box])\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
