{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YAML file saved to /teamspace/studios/this_studio/config.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import os\n",
    "config_data = {\n",
    "    \"paths\": {\n",
    "        \"data_dir\": \"/kaggle/working/\",\n",
    "        \"log_dir\": \"logs\",\n",
    "        \"cache_dir\": \"/kaggle/working/\",\n",
    "        \"checkpoint_dir\": \"/kaggle/working/\"\n",
    "    },\n",
    "    \"wandb\": {\n",
    "        \"using\": False,\n",
    "        \"api_key\": \"your_wandb_api_key\",\n",
    "        \"project\": \"project_name\",\n",
    "        \"run_name_template\": \"{hidden_dim}x{num_hidden_layers}_training\"\n",
    "    },\n",
    "    # Model: convnext_base_w, Pretrained: laion2b_s13b_b82k\n",
    "\n",
    "    \"training\": {\n",
    "        \"batch_size\":4 ,\n",
    "        \"num_epochs\": 10,\n",
    "        \"accumulation_steps\": 2\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"name\": \"ViT-B-16-quickgelu\", # Replace the Model with desired CLIP Model\n",
    "        \"pretrained\": \"openai\", # Corresponding Pretrained Dataset\n",
    "        \"clip_dim\": 512, # Don't forget to change this\n",
    "        \"hidden_dim\": [160],\n",
    "        \"dropout_rate\": [0.15],\n",
    "        \"num_hidden_layers\": [1],\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"clip_lr\": [1e-5],\n",
    "        \"predictor_lr\": [5e-5],\n",
    "        \"weight_decay\": [0.01],\n",
    "        \"beta1\": [0.9],\n",
    "        \"beta2\": [0.999]\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"gamma\": 0.1,\n",
    "        \"milestones\": [4, 6, 10]\n",
    "    }\n",
    "}\n",
    "\n",
    "output_dir = \"/teamspace/studios/this_studio/\"  # Replace with the desired directory\n",
    "file_name = \"config.yml\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Full file path\n",
    "file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# Write the YAML content to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    yaml.dump(config_data, file, default_flow_style=False)\n",
    "\n",
    "print(f\"YAML file saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702b88a529954aea9308ef3cf7a717ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/599M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1549/357239395.py:94: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for CategoryAwareAttributePredictor:\n\tMissing key(s) in state_dict: \"attribute_predictors.defect_fresh.0.weight\", \"attribute_predictors.defect_fresh.0.bias\", \"attribute_predictors.defect_fresh.1.weight\", \"attribute_predictors.defect_fresh.1.bias\", \"attribute_predictors.defect_fresh.4.weight\", \"attribute_predictors.defect_fresh.4.bias\". \n\tUnexpected key(s) in state_dict: \"attribute_predictors.defect_scratch.0.weight\", \"attribute_predictors.defect_scratch.0.bias\", \"attribute_predictors.defect_scratch.1.weight\", \"attribute_predictors.defect_scratch.1.bias\", \"attribute_predictors.defect_scratch.4.weight\", \"attribute_predictors.defect_scratch.4.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary_checkpoint_epoch2.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with actual path\u001b[39;00m\n\u001b[1;32m    126\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path)\n\u001b[0;32m--> 127\u001b[0m clip_model, model, preprocess_val \u001b[38;5;241m=\u001b[39m \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_image\u001b[39m(image):\n\u001b[1;32m    130\u001b[0m     image_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)\n",
      "Cell \u001b[0;32mIn[6], line 95\u001b[0m, in \u001b[0;36mload_models\u001b[0;34m(config, checkpoint_path, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Load checkpoint\u001b[39;00m\n\u001b[1;32m     94\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 95\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m clip_model\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclip_model_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     98\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:2215\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2210\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2211\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2212\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2216\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for CategoryAwareAttributePredictor:\n\tMissing key(s) in state_dict: \"attribute_predictors.defect_fresh.0.weight\", \"attribute_predictors.defect_fresh.0.bias\", \"attribute_predictors.defect_fresh.1.weight\", \"attribute_predictors.defect_fresh.1.bias\", \"attribute_predictors.defect_fresh.4.weight\", \"attribute_predictors.defect_fresh.4.bias\". \n\tUnexpected key(s) in state_dict: \"attribute_predictors.defect_scratch.0.weight\", \"attribute_predictors.defect_scratch.0.bias\", \"attribute_predictors.defect_scratch.1.weight\", \"attribute_predictors.defect_scratch.1.bias\", \"attribute_predictors.defect_scratch.4.weight\", \"attribute_predictors.defect_scratch.4.bias\". "
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import yaml\n",
    "\n",
    "##################\n",
    "# CATEGORY_MAPPING\n",
    "##################\n",
    "CATEGORY_MAPPING = {\n",
    "    \"defect\": {\n",
    "        \"fresh\": \"class\"\n",
    "    }\n",
    "}\n",
    "\n",
    "##################\n",
    "# CategoryAwareAttributePredictor\n",
    "##################\n",
    "class CategoryAwareAttributePredictor(nn.Module):\n",
    "    def __init__(self, clip_dim=768, category_attributes=None, attribute_dims=None, hidden_dim=512, dropout_rate=0.2, num_hidden_layers=1):\n",
    "        super(CategoryAwareAttributePredictor, self).__init__()\n",
    "        self.category_attributes = category_attributes\n",
    "        self.attribute_predictors = nn.ModuleDict()\n",
    "        \n",
    "        for category, attributes in category_attributes.items():\n",
    "            for attr_name in attributes.keys():\n",
    "                key = f\"{category}_{attr_name}\"\n",
    "                if key in attribute_dims:\n",
    "                    layers = []\n",
    "                    # Input layer\n",
    "                    layers.append(nn.Linear(clip_dim, hidden_dim))\n",
    "                    layers.append(nn.LayerNorm(hidden_dim))\n",
    "                    layers.append(nn.ReLU())\n",
    "                    layers.append(nn.Dropout(dropout_rate))\n",
    "                    \n",
    "                    # Additional hidden layers\n",
    "                    current_dim = hidden_dim\n",
    "                    for _ in range(num_hidden_layers - 1):\n",
    "                        layers.append(nn.Linear(current_dim, current_dim // 2))\n",
    "                        layers.append(nn.LayerNorm(current_dim // 2))\n",
    "                        layers.append(nn.ReLU())\n",
    "                        layers.append(nn.Dropout(dropout_rate))\n",
    "                        current_dim = current_dim // 2\n",
    "\n",
    "                    # Output layer\n",
    "                    layers.append(nn.Linear(current_dim, attribute_dims[key]))\n",
    "                    \n",
    "                    self.attribute_predictors[key] = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, clip_features, category):\n",
    "        results = {}\n",
    "        category_attrs = self.category_attributes[category]\n",
    "        clip_features = clip_features.float()\n",
    "        \n",
    "        for attr_name in category_attrs.keys():\n",
    "            key = f\"{category}_{attr_name}\"\n",
    "            if key in self.attribute_predictors:\n",
    "                results[key] = self.attribute_predictors[key](clip_features)\n",
    "        \n",
    "        return results\n",
    "\n",
    "##################\n",
    "# Helper Functions\n",
    "##################\n",
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def load_models(config, checkpoint_path, device):\n",
    "    # Create CLIP model and transforms\n",
    "    clip_model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms(\n",
    "        config['model']['name'],\n",
    "        pretrained=config['model']['pretrained'],\n",
    "        device=device\n",
    "    )\n",
    "    clip_model = clip_model.float()\n",
    "    \n",
    "    # Define attribute_dims (binary classification: 2 classes)\n",
    "    attribute_dims = {\"defect_fresh\": 2}\n",
    "    \n",
    "    model = CategoryAwareAttributePredictor(\n",
    "        clip_dim=config['model']['clip_dim'],\n",
    "        category_attributes=CATEGORY_MAPPING,\n",
    "        attribute_dims=attribute_dims,\n",
    "        hidden_dim=config['model']['hidden_dim'][0],\n",
    "        dropout_rate=config['model']['dropout_rate'][0],\n",
    "        num_hidden_layers=config['model']['num_hidden_layers'][0]\n",
    "    ).to(device)\n",
    "\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    clip_model.load_state_dict(checkpoint['clip_model_state_dict'])\n",
    "\n",
    "    model.eval()\n",
    "    clip_model.eval()\n",
    "    \n",
    "    return clip_model, model, preprocess_val\n",
    "\n",
    "def infer_image(clip_model, model, preprocess, image, device):\n",
    "    image_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "    category = \"defect\"  # known from training\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.encode_image(image_tensor)\n",
    "        predictions = model(image_features, category)\n",
    "        logits = predictions[\"defect_fresh\"]  # shape [1, 2]\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        class_names = [\"rotten\", \"fresh\"]\n",
    "        pred_label = class_names[pred_class]\n",
    "        \n",
    "        return pred_label, probs.cpu().numpy()\n",
    "\n",
    "##################\n",
    "# Initialization\n",
    "##################\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config_path = \"config.yml\"  # Replace with actual path\n",
    "checkpoint_path = \"/kaggle/working/binary_checkpoint_epoch1.pth\"  # Replace with actual path\n",
    "\n",
    "config = load_config(config_path)\n",
    "clip_model, model, preprocess_val = load_models(config, checkpoint_path, device)\n",
    "\n",
    "def process_image(image):\n",
    "    image_pil = Image.fromarray(image)\n",
    "    pred_label, probs = infer_image(clip_model, model, preprocess_val, image_pil, device)\n",
    "    return f\"Predicted Label: {pred_label}\\nProbabilities: {probs}\"\n",
    "\n",
    "##################\n",
    "# Gradio Integration\n",
    "##################\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Fruit Freshness Detection\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Upload an image of a fruit**, and the model will predict whether the fruit is fresh or rotten.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")\n",
    "            submit_button = gr.Button(\"Run Inference\")\n",
    "\n",
    "        with gr.Column():\n",
    "            output_box = gr.Textbox(label=\"Prediction\", lines=3, max_lines=5)\n",
    "\n",
    "    submit_button.click(fn=process_image, inputs=image_input, outputs=[output_box])\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
