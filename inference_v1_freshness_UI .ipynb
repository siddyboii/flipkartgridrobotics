{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "`Qwen2VLRotaryEmbedding` can now be fully parameterized by passing the model config through the `config` argument. All other arguments will be removed in v4.46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c97b45cb8ca9466ab8ce877bb64c9ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "# Model and adapter paths\n",
    "adapter_path = \"/teamspace/studios/this_studio/qwen2-7b-instruct-freshness_detecter_v1/checkpoint-155\"\n",
    "\n",
    "# Set up Qwen model\n",
    "model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/teamspace/studios/this_studio/qwen2-7b-instruct-freshness_detecter_v1\"\n",
    ")\n",
    "\n",
    "# Processor setup\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    \"Qwen/Qwen2-VL-2B-Instruct\", \n",
    "    cache_dir=\"/teamspace/studios/this_studio/qwen2-7b-instruct-freshness_detecter_v1\", \n",
    "    max_pixels=720 * 28 * 28\n",
    ")\n",
    "\n",
    "# Load adapter\n",
    "model.load_adapter(adapter_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7865\n",
      "* Running on public URL: https://5b5bebf247b6e9bf03.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://5b5bebf247b6e9bf03.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# Function to process the image and assess freshness\n",
    "def process_freshness(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(np.uint8(image))\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\", \n",
    "                    \"image\": pil_image  # Use the processed PIL image\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Identify the fruit or vegetable in the image and strictly categorize its freshness as one of the following: 'extreme freshness,' 'mild freshness,' or 'worse.' The assessment should be based on observable factors such as color, texture, and signs of spoilage. Make sure to return only one of these three categories in your response, along with a brief explanation of why this category was chosen.\"\n",
    "                    # \"text\": \"Identify the fruit or vegetable in the image and strictly assess its freshness as 'extreme freshness,' 'mild freshness,' or 'worse' based on color, texture, and signs of spoilage. Provide a brief explanation for your assessment.\"\n",
    "                }   \n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    # \"Identify the fruit or vegetable in the image and strictly categorize its freshness as one of the following: 'extreme freshness,' 'mild freshness,' or 'worse.' The assessment should be based on observable factors such as color, texture, and signs of spoilage. Make sure to return only one of these three categories in your response, along with a brief explanation of why this category was chosen.\"\n",
    "\n",
    "    \n",
    "    # Prepare input for inference\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output from the Qwen model\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return output_text[0]  # Return the output as a string\n",
    "\n",
    "# Gradio UI\n",
    "def run_gradio_interface(image):\n",
    "    output_text = process_freshness(image)\n",
    "    return output_text\n",
    "\n",
    "# Define Gradio interface with image input and text output\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"## Freshness Detection with Qwen Model\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Upload an image of a fruit or vegetable**, and the Qwen model will identify the item and assess its freshness.\n",
    "        The assessment will include categories such as 'extreme freshness,' 'mild freshness,' or 'worse,' with a brief explanation.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define the layout for the interface\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")  # Image upload box\n",
    "            submit_button = gr.Button(\"Run Freshness Detection\")  # Submit button\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_box = gr.Textbox(label=\"Freshness Detection Output\", lines=10, max_lines=20)  # Text output\n",
    "\n",
    "    # Add interaction to trigger inference\n",
    "    submit_button.click(fn=run_gradio_interface, inputs=image_input, outputs=output_box)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://8d78f2129fa656a4f4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://8d78f2129fa656a4f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    Exception in ASGI application\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/uvicorn/protocols/http/h11_impl.py\", line 406, in run_asgi\n",
      "    result = await app(  # type: ignore[func-returns-value]\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\n",
      "    return await self.app(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\n",
      "    await super().__call__(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/applications.py\", line 113, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n",
      "    raise exc\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n",
      "    await self.app(scope, receive, _send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/route_utils.py\", line 796, in __call__\n",
      "    await self.simple_response(scope, receive, send, request_headers=headers)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/route_utils.py\", line 812, in simple_response\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n",
      "    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/routing.py\", line 715, in __call__\n",
      "    await self.middleware_stack(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/routing.py\", line 735, in app\n",
      "    await route.handle(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/routing.py\", line 288, in handle\n",
      "    await self.app(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/routing.py\", line 76, in app\n",
      "    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\n",
      "    raise exc\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\n",
      "    await app(scope, receive, sender)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/routing.py\", line 73, in app\n",
      "    response = await f(request)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/fastapi/routing.py\", line 301, in app\n",
      "    raw_response = await run_endpoint_function(\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n",
      "    return await dependant.call(**values)\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/routes.py\", line 1369, in upload_file\n",
      "    form = await multipart_parser.parse()\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/gradio/route_utils.py\", line 650, in parse\n",
      "    async for chunk in self.stream:\n",
      "  File \"/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/starlette/requests.py\", line 228, in stream\n",
      "    raise ClientDisconnect()\n",
      "starlette.requests.ClientDisconnect\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "# Shelf life mapping based on fruit type and freshness\n",
    "def get_shelf_life(fruit_type, freshness):\n",
    "    shelf_life_map = {\n",
    "    'apple': {\n",
    "        'extreme freshness': 'around 7 days',\n",
    "        'mild freshness': 'around 3-5 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    'banana': {\n",
    "        'extreme freshness': 'around 4 days',\n",
    "        'mild freshness': 'around 2-3 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    'potato': {\n",
    "        'extreme freshness': 'around 45-60 days',\n",
    "        'mild freshness': 'around 10-12 days',\n",
    "        'worse': 'less than 3 days'\n",
    "    },\n",
    "    'orange': {\n",
    "        'extreme freshness': 'around 7 days',\n",
    "        'mild freshness': 'around 3-5 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    'okra': {\n",
    "        'extreme freshness': 'around 7-10 days',\n",
    "        'mild freshness': 'around 4-6 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    'tomato': {\n",
    "        'extreme freshness': 'around 4-6 days',\n",
    "        'mild freshness': 'around 2-3 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    'cucumber': {\n",
    "        'extreme freshness': 'around 7 days',\n",
    "        'mild freshness': 'around 3-5 days',\n",
    "        'worse': 'less than 2 days'\n",
    "    },\n",
    "    # Add more fruits or vegetables here\n",
    "}\n",
    "\n",
    "    fruit_type = fruit_type.lower()\n",
    "    freshness = freshness.lower()\n",
    "\n",
    "    if fruit_type in shelf_life_map and freshness in shelf_life_map[fruit_type]:\n",
    "        return shelf_life_map[fruit_type][freshness]\n",
    "    else:\n",
    "        return \"Unknown shelf life\"\n",
    "\n",
    "# Function to process the image and assess freshness with shelf life\n",
    "def process_freshness(image):\n",
    "    # Convert numpy array to PIL Image\n",
    "    pil_image = Image.fromarray(np.uint8(image))\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\", \n",
    "                    \"image\": pil_image  # Use the processed PIL image\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": \"Identify the fruit or vegetable in the image and strictly categorize its freshness as one of the following: 'extreme freshness,' 'mild freshness,' or 'worse.' The assessment should be based on observable factors such as color, texture, and signs of spoilage. Make sure to return only one of these three categories in your response, along with a brief explanation of why this category was chosen.\"\n",
    "                }   \n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Prepare input for inference\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output from the Qwen model\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=64)\n",
    "    generated_ids_trimmed = [out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "    # Post-process the output to extract fruit type and freshness\n",
    "    fruit_type = \"unknown\"\n",
    "    freshness = \"unknown\"\n",
    "\n",
    "    if \"apple\" in output_text.lower():\n",
    "        fruit_type = \"apple\"\n",
    "    elif \"banana\" in output_text.lower():\n",
    "        fruit_type = \"banana\"\n",
    "    elif \"potato\" in output_text.lower():\n",
    "        fruit_type = \"potato\"\n",
    "    elif \"orange\" in output_text.lower():\n",
    "        fruit_type = \"orange\"\n",
    "    elif \"okra\" in output_text.lower():\n",
    "        fruit_type = \"okra\"\n",
    "    elif \"tomato\" in output_text.lower():\n",
    "        fruit_type = \"tomato\"\n",
    "    elif \"cucumber\" in output_text.lower():\n",
    "        fruit_type = \"cucumber\"\n",
    "    # Add detection logic for other fruits/vegetables here as needed\n",
    "\n",
    "    if \"extreme freshness\" in output_text.lower():\n",
    "        freshness = \"extreme freshness\"\n",
    "    elif \"mild freshness\" in output_text.lower():\n",
    "        freshness = \"mild freshness\"\n",
    "    elif \"moderate freshness\" in output_text.lower():\n",
    "        freshness = \"mild freshness\"\n",
    "    elif \"worse\" in output_text.lower():\n",
    "        freshness = \"worse\"\n",
    "\n",
    "    # Get estimated shelf life based on fruit type and freshness\n",
    "    shelf_life = get_shelf_life(fruit_type, freshness)\n",
    "\n",
    "    # Append shelf life information to the output\n",
    "    output_text += f\"\\n\\nEstimated Shelf Life for {fruit_type.capitalize()} with {freshness.capitalize()}: {shelf_life}\"\n",
    "\n",
    "    return output_text\n",
    "\n",
    "# Gradio UI\n",
    "def run_gradio_interface(image):\n",
    "    output_text = process_freshness(image)\n",
    "    return output_text\n",
    "\n",
    "# Define Gradio interface with image input and text output\n",
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    # Add a title and description\n",
    "    gr.Markdown(\"## Freshness Detection with Shelf Life Estimation\")\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        **Upload an image of a fruit or vegetable**, and the Qwen model will identify the item and assess its freshness.\n",
    "        Based on the freshness level, the app will also estimate the shelf life of the fruit or vegetable.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define the layout for the interface\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            image_input = gr.Image(type=\"numpy\", label=\"Upload Image\")  # Image upload box\n",
    "            submit_button = gr.Button(\"Run Freshness Detection\")  # Submit button\n",
    "        \n",
    "        with gr.Column():\n",
    "            output_box = gr.Textbox(label=\"Freshness Detection Output\", lines=10, max_lines=20)  # Text output\n",
    "\n",
    "    # Add interaction to trigger inference\n",
    "    submit_button.click(fn=run_gradio_interface, inputs=image_input, outputs=output_box)\n",
    "\n",
    "# Launch the app\n",
    "demo.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
